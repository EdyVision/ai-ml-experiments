{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10876989,"datasetId":6758176,"databundleVersionId":11243363}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuned LlaMa 3.2 3B - Financial Aid Usecase \n\nThis notebook pulls down the unsloth LlaMa 3.2 3B model from HuggingFace and leverages a pre-made synthetic dataset comprising of fictitious student records. LoRA adapters were used to fine-tune the model efficiently, reducing memory usage by keeping the base model frozen and training only additional adapter layers. No real data is used in this dataset and the code is free from any conflict of interest to the best of the author's knowledge. This author assumes the user has a HuggingFace token and has set their GPU setting to T4.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Uncomment the following, run, then comment and restart session (do not factory reset)","metadata":{}},{"cell_type":"code","source":"# Run this first, then comment out and restart session\n# !pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n# !pip install -q -U trl\n# !pip install -q -U peft\n# !pip install faker","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Install dependencies and deal with memory settings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport torch\nimport random\nfrom torch.utils.data import Dataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\n# For LLM\nfrom peft import LoraConfig, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed,\n    pipeline\n)\nfrom trl import SFTTrainer, setup_chat_format, SFTConfig\n\nfrom time import time\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:07.025302Z","iopub.execute_input":"2025-02-28T20:44:07.025608Z","iopub.status.idle":"2025-02-28T20:44:14.767623Z","shell.execute_reply.started":"2025-02-28T20:44:07.025585Z","shell.execute_reply":"2025-02-28T20:44:14.766946Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Import HuggingFace Secret\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_label = \"HUGGINGFACE_TOKEN\"\nsecret_value = UserSecretsClient().get_secret(secret_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:14.768576Z","iopub.execute_input":"2025-02-28T20:44:14.768825Z","iopub.status.idle":"2025-02-28T20:44:14.972146Z","shell.execute_reply.started":"2025-02-28T20:44:14.768803Z","shell.execute_reply":"2025-02-28T20:44:14.971557Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# The following helps with the CUDA memory issue\ntorch.cuda.empty_cache()\ntorch.cuda.reset_max_memory_allocated()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:14.973527Z","iopub.execute_input":"2025-02-28T20:44:14.973802Z","iopub.status.idle":"2025-02-28T20:44:15.057514Z","shell.execute_reply.started":"2025-02-28T20:44:14.973779Z","shell.execute_reply":"2025-02-28T20:44:15.056757Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 3: Load Datasets and Synthetic Database","metadata":{}},{"cell_type":"code","source":"# Load Data to Fine Tune LlaMa\nroot_path = '/kaggle/input/student-chat-data/student_chat_ai_data'\ntrain_df = pd.read_csv(f\"{root_path}/train_data.csv\", index_col=0)\neval_df = pd.read_csv(f\"{root_path}/eval_data.csv\", index_col=0)\nsynthetic_student_population_df = pd.read_csv(f\"{root_path}/synthetic_population_data.csv\", index_col=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:15.058340Z","iopub.execute_input":"2025-02-28T20:44:15.058623Z","iopub.status.idle":"2025-02-28T20:44:15.073394Z","shell.execute_reply.started":"2025-02-28T20:44:15.058599Z","shell.execute_reply":"2025-02-28T20:44:15.072605Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:16.019805Z","iopub.execute_input":"2025-02-28T20:44:16.020110Z","iopub.status.idle":"2025-02-28T20:44:16.030843Z","shell.execute_reply.started":"2025-02-28T20:44:16.020086Z","shell.execute_reply":"2025-02-28T20:44:16.030002Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                             context  \\\n0  Student Linda Bailey (ID: S00001) has a GPA of...   \n1  Student John Doe (ID: S00002) has a GPA of 3.8...   \n2  Student Sarah Carter (ID: S00003) has a GPA of...   \n3  Student John Doe (ID: S10123) has a GPA of 2.3...   \n\n                                    user_input  \\\n0         What financial aid do I qualify for?   \n1        What financial aid am I eligible for?   \n2           Can I apply for any financial aid?   \n3  What am I to be eligible for financial aid?   \n\n                                              answer  \n0  You qualify for the Need-Based Grant because y...  \n1  John Doe qualifies for the Merit-Based Scholar...  \n2  Yes! You qualify for the STEM Excellence Award...  \n3  Due to your income, you do not qualify for a n...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>user_input</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Student Linda Bailey (ID: S00001) has a GPA of...</td>\n      <td>What financial aid do I qualify for?</td>\n      <td>You qualify for the Need-Based Grant because y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Student John Doe (ID: S00002) has a GPA of 3.8...</td>\n      <td>What financial aid am I eligible for?</td>\n      <td>John Doe qualifies for the Merit-Based Scholar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Student Sarah Carter (ID: S00003) has a GPA of...</td>\n      <td>Can I apply for any financial aid?</td>\n      <td>Yes! You qualify for the STEM Excellence Award...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Student John Doe (ID: S10123) has a GPA of 2.3...</td>\n      <td>What am I to be eligible for financial aid?</td>\n      <td>Due to your income, you do not qualify for a n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"eval_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:17.779248Z","iopub.execute_input":"2025-02-28T20:44:17.779584Z","iopub.status.idle":"2025-02-28T20:44:17.787570Z","shell.execute_reply.started":"2025-02-28T20:44:17.779555Z","shell.execute_reply":"2025-02-28T20:44:17.786707Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                             context  \\\n0  Student Emily Carter (ID: S00023) has a GPA of...   \n1  Student Mark Smith (ID: S00045) has a GPA of 2...   \n2  Student Olivia Jones (ID: S00030) has a GPA of...   \n3  Student Daniel Thompson (ID: S00051) has a GPA...   \n\n                                      user_input  \\\n0           Am I eligible for any financial aid?   \n1  Can I get coverage with my education program?   \n2      Which financial aid can do I qualify for?   \n3          What financial aid options do I have?   \n\n                                              answer  \n0  Yes, you qualify for the STEM Excellence Award...  \n1  Unfortunately, you do not qualify for any fina...  \n2  You qualify for the STEM Excellence Award sinc...  \n3  You are eligible for the Need-Based Grant sinc...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>user_input</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Student Emily Carter (ID: S00023) has a GPA of...</td>\n      <td>Am I eligible for any financial aid?</td>\n      <td>Yes, you qualify for the STEM Excellence Award...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Student Mark Smith (ID: S00045) has a GPA of 2...</td>\n      <td>Can I get coverage with my education program?</td>\n      <td>Unfortunately, you do not qualify for any fina...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Student Olivia Jones (ID: S00030) has a GPA of...</td>\n      <td>Which financial aid can do I qualify for?</td>\n      <td>You qualify for the STEM Excellence Award sinc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Student Daniel Thompson (ID: S00051) has a GPA...</td>\n      <td>What financial aid options do I have?</td>\n      <td>You are eligible for the Need-Based Grant sinc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"synthetic_student_population_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:19.256803Z","iopub.execute_input":"2025-02-28T20:44:19.257118Z","iopub.status.idle":"2025-02-28T20:44:19.267573Z","shell.execute_reply.started":"2025-02-28T20:44:19.257093Z","shell.execute_reply":"2025-02-28T20:44:19.266846Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         id              name   gpa           field_of_study  income\n0    S00001      Marc Solomon  2.53             Anthropology   73706\n1    S00002      Suzanne Kidd  2.50  Business Administration   22259\n2    S00003  Michelle Griffin  2.42        Political Science   31192\n3    S00004      Hannah Dixon  2.05              Engineering  530431\n4    S00005       Mary Harris  3.13                Chemistry  370142\n..      ...               ...   ...                      ...     ...\n995  S00996       Nicole Clay  3.97                Chemistry  353792\n996  S00997        Tammy Rose  3.34         Computer Science  414283\n997  S00998      Jeff Alvarez  3.96               Psychology  450287\n998  S00999       Thomas Lowe  3.02  Business Administration   54775\n999  S01000   Veronica Barker  3.29             Anthropology  270061\n\n[1000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>gpa</th>\n      <th>field_of_study</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S00001</td>\n      <td>Marc Solomon</td>\n      <td>2.53</td>\n      <td>Anthropology</td>\n      <td>73706</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>S00002</td>\n      <td>Suzanne Kidd</td>\n      <td>2.50</td>\n      <td>Business Administration</td>\n      <td>22259</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S00003</td>\n      <td>Michelle Griffin</td>\n      <td>2.42</td>\n      <td>Political Science</td>\n      <td>31192</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>S00004</td>\n      <td>Hannah Dixon</td>\n      <td>2.05</td>\n      <td>Engineering</td>\n      <td>530431</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>S00005</td>\n      <td>Mary Harris</td>\n      <td>3.13</td>\n      <td>Chemistry</td>\n      <td>370142</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>S00996</td>\n      <td>Nicole Clay</td>\n      <td>3.97</td>\n      <td>Chemistry</td>\n      <td>353792</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>S00997</td>\n      <td>Tammy Rose</td>\n      <td>3.34</td>\n      <td>Computer Science</td>\n      <td>414283</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>S00998</td>\n      <td>Jeff Alvarez</td>\n      <td>3.96</td>\n      <td>Psychology</td>\n      <td>450287</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>S00999</td>\n      <td>Thomas Lowe</td>\n      <td>3.02</td>\n      <td>Business Administration</td>\n      <td>54775</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>S01000</td>\n      <td>Veronica Barker</td>\n      <td>3.29</td>\n      <td>Anthropology</td>\n      <td>270061</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class StudentDialogueDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        prompt = f\"### Role: You are an AI financial aid evaluator. Answer the user's question based on the provided student's profile.\\n\\n### Context:\\n{item['context']}\\n\\n### User's Question:\\n{item['user_input']}\\n\\n###Answer:\"\n        target = item[\"answer\"]\n        encoding = self.tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        target_encoding = self.tokenizer(target, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        \n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": target_encoding[\"input_ids\"].squeeze(0),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:21.438263Z","iopub.execute_input":"2025-02-28T20:44:21.438733Z","iopub.status.idle":"2025-02-28T20:44:21.446109Z","shell.execute_reply.started":"2025-02-28T20:44:21.438682Z","shell.execute_reply":"2025-02-28T20:44:21.445092Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Step 4: Load LlaMa 3 8B Model and Train","metadata":{}},{"cell_type":"markdown","source":"#### If BitsAndBytes 8-bit quantization error pops up requiring installation and you already ran the first cell, simply restart the session (do NOT factory reset)","metadata":{}},{"cell_type":"code","source":"\n# Set the data type for computations to float16, bfloat16 not supported on T4/P100\ncompute_dtype = getattr(torch, \"float16\")\n\n# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n    bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n)\n\n# Import Base Model\nmodel_name = \"Llama-3.2-3B\"\nmodel_path = \"unsloth/Llama-3.2-3B\"\nbase_model_name = re.sub(r\"-\", \"_\", model_name)\nstorage_path = f\"./dist/{base_model_name.lower()}/{base_model_name.lower()}_base.pth\"\n\n# Record the start time to measure the loading duration\ntime_start = time()\n\n# Load the pre-trained model with specified configurations\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=compute_dtype,  # Set the data type for the model\n    load_in_8bit=True,  # 8-bit quantization to fit in 14GB\n    use_cache=False,  # Disable caching to save memory\n    device_map='auto',  # Automatically map the model to available devices (e.g., GPUs)\n    token=secret_value\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:44:24.404423Z","iopub.execute_input":"2025-02-28T20:44:24.404784Z","iopub.status.idle":"2025-02-28T20:45:02.100261Z","shell.execute_reply.started":"2025-02-28T20:44:24.404754Z","shell.execute_reply":"2025-02-28T20:45:02.099315Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee1938db30c44bdaee31f892bc3a48e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d92e9492214007982c37ed16bef140"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from peft import get_peft_model\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,  # Rank (smaller = more efficient)\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"  # For language modeling\n)\n\n# Apply LoRA to 8-bit model\nmodel = get_peft_model(model, lora_config)\n\n# After applying LoRA, confirm which layers are trainable\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:45:28.844028Z","iopub.execute_input":"2025-02-28T20:45:28.844358Z","iopub.status.idle":"2025-02-28T20:45:28.951539Z","shell.execute_reply.started":"2025-02-28T20:45:28.844335Z","shell.execute_reply":"2025-02-28T20:45:28.950620Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# Load the tokenizer associated with the model\ntokenizer = AutoTokenizer.from_pretrained(model_path, token=secret_value)\ntokenizer.pad_token = tokenizer.eos_token  # Set the padding token to the end-of-sequence token you could also introduce a special pad token but this is not needed.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:45:31.289357Z","iopub.execute_input":"2025-02-28T20:45:31.289689Z","iopub.status.idle":"2025-02-28T20:45:32.895903Z","shell.execute_reply.started":"2025-02-28T20:45:31.289665Z","shell.execute_reply":"2025-02-28T20:45:32.895149Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba599d59ce04e389b13e6e909b92b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98bb9b37b6cf48eeaba73984be5d8a16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace3e02dd8814c9bb67499d564653dc4"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_dataset = StudentDialogueDataset(train_df.to_dict(orient='records'), tokenizer)\neval_dataset = StudentDialogueDataset(eval_df.to_dict(orient='records'), tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:45:34.120991Z","iopub.execute_input":"2025-02-28T20:45:34.121337Z","iopub.status.idle":"2025-02-28T20:45:34.130460Z","shell.execute_reply.started":"2025-02-28T20:45:34.121305Z","shell.execute_reply":"2025-02-28T20:45:34.129639Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Set Model to Train\nmodel.train()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./llama3-kaggle-checkpoints\",\n    num_train_epochs=3,  # Keep low to avoid timeouts\n    per_device_train_batch_size=1,  # Keep batch size low\n    gradient_accumulation_steps=8,  # Helps with small batch size\n    learning_rate=5e-6,\n    save_strategy=\"no\",  # Avoid saving large model checkpoints\n    logging_steps=10,\n    fp16=True,  # Enable mixed precision for lower memory\n    optim=\"adamw_torch\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Your tokenized dataset\n    tokenizer=tokenizer\n)\n\n# Train the Model\ntrainer.train()\n\n# Save LoRa Adapter Files\ntrainer.model.save_pretrained(\"./llama3-kaggle-checkpoints\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:45:38.534500Z","iopub.execute_input":"2025-02-28T20:45:38.534861Z","iopub.status.idle":"2025-02-28T20:45:47.154630Z","shell.execute_reply.started":"2025-02-28T20:45:38.534830Z","shell.execute_reply":"2025-02-28T20:45:47.153728Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-13-aca18a2e1c7a>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Step 5: Preprocess Synthetic Student Population Data\nUsing the synthetic population dataset, construct prompts to test out the model's performance","metadata":{}},{"cell_type":"code","source":"fake_students = synthetic_student_population_df.to_dict(orient=\"records\")\n\nprint(f\"{len(fake_students)} student records available\")\nprint(\"\\nData record shape:\\n\")\nprint(fake_students[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:47:50.055511Z","iopub.execute_input":"2025-02-28T20:47:50.055858Z","iopub.status.idle":"2025-02-28T20:47:50.064590Z","shell.execute_reply.started":"2025-02-28T20:47:50.055829Z","shell.execute_reply":"2025-02-28T20:47:50.063750Z"}},"outputs":[{"name":"stdout","text":"1000 student records available\n\nData record shape:\n\n{'id': 'S00001', 'name': 'Marc Solomon', 'gpa': 2.53, 'field_of_study': 'Anthropology', 'income': 73706}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from typing import Optional\n\n@staticmethod\ndef determine_financial_aid_eligibility(gpa, field_of_study, income):\n    \"\"\"\n    Uses a student record to determine coverage against some fictitious\n    parameters\n    \"\"\"\n    # Dummy logic for financial aid eligibility\n    financial_aid = []\n    requirements = []\n\n    if gpa >= 3.6:\n        requirements.append(\"GPA ≥ 3.6\")\n    if income <= 35000:\n        requirements.append(\"Income ≤ $35,000\")\n    if field_of_study in [\n        \"Computer Science\",\n        \"Engineering\",\n        \"Mathematics\",\n        \"IT\",\n        \"Statistics\",\n    ]:\n        financial_aid.append(\"STEM Excellence Award\")\n        requirements.append(\"Field of Study in STEM\")\n    if field_of_study in [\"Psychology\", \"Sociology\", \"Social Work\", \"Anthropology\"]:\n        financial_aid.append(\"Behavioral and Social Sciences Grant\")\n        requirements.append(\"Field of Study in Behavioral and Social Sciences\")\n    if field_of_study in [\"Paralegal\", \"Law\"]:\n        financial_aid.append(\"Legal Studies Full Ride\")\n        requirements.append(\"Field of Study in Legal Studies\")\n\n    return requirements, financial_aid\n\ndef retrieve_financial_aid_knowledge(student_id: Optional[str] = None, student_name: Optional[str] = None):\n    \"\"\"\n    Retrieves financial aid eligibility knowledge for a student.\n    \"\"\"\n    if student_id:\n        student = next((item for item in fake_members if item[\"id\"] == student_id), None)\n    if student_name:\n        student = next((item for item in fake_members if item[\"name\"] == student_name), None)\n\n    if not student:\n        return None\n\n    requirements, financial_aid = determine_financial_aid_eligibility(\n        student[\"gpa\"], student[\"field_of_study\"], student[\"income\"]\n        )\n        \n\n    return {\n        \"id\": student[\"id\"],\n        \"name\": student[\"name\"],\n        \"gpa\": student[\"gpa\"],\n        \"field_of_study\": student[\"field_of_study\"],\n        \"income\": student[\"income\"],\n        \"financial_aid\": financial_aid,\n        \"requirements\": requirements,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:49:30.116783Z","iopub.execute_input":"2025-02-28T20:49:30.117116Z","iopub.status.idle":"2025-02-28T20:49:30.124595Z","shell.execute_reply.started":"2025-02-28T20:49:30.117093Z","shell.execute_reply":"2025-02-28T20:49:30.123604Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def format(student: dict) -> str:\n    \"\"\"\n    Converts the retrieved student and financial aid knowledge into an LLM-readable prompt.\n    \"\"\"\n    return f\"Recipient {student['name']} (ID: {student['id']}) has a GPA of {student['gpa']} in {student['field_of_study']} \" + \\\n        f\"and an income of ${student['income']}. \" + \\\n        f\"Eligible financial aid: {', '.join(student['financial_aid']) if student.get('financial_aid') else 'None'}. \" + \\\n        f\"Requirements met: {', '.join(student['requirements']) if student.get('requirements') else 'None'}.\"\n\n\nretrieved_knowledge = retrieve_financial_aid_knowledge(\"S00001\")\nprint(retrieved_knowledge)  # Data as-is\nprint(format(retrieved_knowledge)) # Data formatted for LLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:01:19.393197Z","iopub.execute_input":"2025-02-28T21:01:19.393542Z","iopub.status.idle":"2025-02-28T21:01:19.399775Z","shell.execute_reply.started":"2025-02-28T21:01:19.393516Z","shell.execute_reply":"2025-02-28T21:01:19.398958Z"}},"outputs":[{"name":"stdout","text":"{'id': 'S00001', 'name': 'Tracy Page', 'gpa': 3.28, 'field_of_study': 'Computer Science', 'income': 797572, 'financial_aid': ['STEM Excellence Award'], 'requirements': ['Field of Study in STEM']}\nRecipient Tracy Page (ID: S00001) has a GPA of 3.28 in Computer Science and an income of $797572. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Step 6: Pre-build all prompts using synthetic population data","metadata":{}},{"cell_type":"code","source":"from typing import Optional, Tuple\n\ndef build_student_prompt(user_input: str, student_id: Optional[str] = None) -> Tuple[str, str]:\n    \"\"\"\n    Builds student chat prompt for LLM and uses RAG to keep responses properly informed for\n    the given student with student_id.\n    \"\"\"\n\n    # student id available and valid, proceed with lookup\n    knowledge = format(retrieve_financial_aid_knowledge(student_id))\n\n    # Retrun both the knowledge rertieved and the prompt\n    return (\n        knowledge,\n        f\"\"\"\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\n{knowledge}\\n\\n### User's Question:\\n{user_input}\\n\\n###Answer: \n        \"\"\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:09:50.037805Z","iopub.execute_input":"2025-02-28T21:09:50.038146Z","iopub.status.idle":"2025-02-28T21:09:50.042828Z","shell.execute_reply.started":"2025-02-28T21:09:50.038120Z","shell.execute_reply":"2025-02-28T21:09:50.041875Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# Define test dataset (Example questions)\ndef build_test_sentences():\n    test_sentences = [\n        f\"What GPA do I need need to qualify for financial aid?\",\n        f\"Am I eligible for scholarships?\",\n        f\"What grants can I apply for?\",\n    ]\n\n    return test_sentences\n\nstudent_prompts = []\n\nfor student in fake_students:\n    test_sentence = random.choice(build_test_sentences())\n    knowledge_retrieved, prompt = build_student_prompt(user_input=test_sentence, student_id=student[\"id\"])\n    student_prompts.append(prompt)\n\nstudent_prompts[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:09:51.830147Z","iopub.execute_input":"2025-02-28T21:09:51.830451Z","iopub.status.idle":"2025-02-28T21:09:51.869571Z","shell.execute_reply.started":"2025-02-28T21:09:51.830429Z","shell.execute_reply":"2025-02-28T21:09:51.868597Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"[\"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient Tracy Page (ID: S00001) has a GPA of 3.28 in Computer Science and an income of $797572. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \",\n \"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient David Nguyen (ID: S00002) has a GPA of 2.55 in Political Science and an income of $166316. Eligible financial aid: None. Requirements met: None.\\n\\n### User's Question:\\nWhat grants can I apply for?\\n\\n###Answer: \\n        \",\n \"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient Brenda Simmons (ID: S00003) has a GPA of 3.47 in Biology and an income of $639176. Eligible financial aid: None. Requirements met: None.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \",\n \"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient Preston Marshall (ID: S00004) has a GPA of 2.84 in Computer Science and an income of $118246. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\\n\\n### User's Question:\\nWhat grants can I apply for?\\n\\n###Answer: \\n        \",\n \"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient Ann Gray (ID: S00005) has a GPA of 2.44 in Computer Science and an income of $608508. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \"]"},"metadata":{}}],"execution_count":60},{"cell_type":"markdown","source":"#### Set to start testing!","metadata":{}},{"cell_type":"markdown","source":"## Step 7: Run custom student inferences","metadata":{}},{"cell_type":"code","source":"# Insert custom inference\ncheckpoints_dir = \"./llama3-kaggle-checkpoints\"\n\n# Enable CUDA if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load base LLaMA 3 3B model in 8-bit mode\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    load_in_8bit=True,  # Load in 8-bit mode\n    device_map=\"auto\",  # Automatically assigns to GPU\n    token=secret_value\n)\n\n# Load fine-tuned LoRA adapter (Attaches the fine-tuned LoRA adapter to the base model)\nmodel = PeftModel.from_pretrained(model, checkpoints_dir)\nmodel = torch.compile(model) # To boost inference speed\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, token=secret_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:06:26.908857Z","iopub.execute_input":"2025-02-28T21:06:26.909172Z","iopub.status.idle":"2025-02-28T21:06:35.443679Z","shell.execute_reply.started":"2025-02-28T21:06:26.909146Z","shell.execute_reply":"2025-02-28T21:06:35.442941Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Select first prompt\nfirst_prompt = random.choice(student_prompts)\nfirst_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:09:56.181969Z","iopub.execute_input":"2025-02-28T21:09:56.182272Z","iopub.status.idle":"2025-02-28T21:09:56.188895Z","shell.execute_reply.started":"2025-02-28T21:09:56.182248Z","shell.execute_reply":"2025-02-28T21:09:56.187828Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"\"\\n        ### Role: You are an AI academic benefits evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nRecipient Jasmine Randall (ID: S00989) has a GPA of 3.01 in Anthropology and an income of $464150. Eligible financial aid: Behavioral and Social Sciences Grant. Requirements met: Field of Study in Behavioral and Social Sciences.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \""},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"def extract_answer(response_text):\n    \"\"\"\n    Extracts the generated answer from the LLaMA response.\n    \"\"\"\n\n    # Match the last \"Answer:\" followed by actual content\n    match = re.search(\n        r\"(?i)(?:Your response:|Answer:)\\s*\\n*(.*?)(?=\\n\\n|\\Z)\",\n        response_text,\n        re.DOTALL,\n    )\n\n    if match:\n        extracted_answer = match.group(1).strip()\n        return extracted_answer\n    else:\n        return \"No answer found.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:10:15.057003Z","iopub.execute_input":"2025-02-28T21:10:15.057324Z","iopub.status.idle":"2025-02-28T21:10:15.061475Z","shell.execute_reply.started":"2025-02-28T21:10:15.057300Z","shell.execute_reply":"2025-02-28T21:10:15.060678Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"def generate_text(prompt, max_new_tokens=100):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Run first prompt\nresponse = generate_text(first_prompt)\nprint(extract_answer(response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:10:16.312257Z","iopub.execute_input":"2025-02-28T21:10:16.312551Z","iopub.status.idle":"2025-02-28T21:10:28.148053Z","shell.execute_reply.started":"2025-02-28T21:10:16.312527Z","shell.execute_reply":"2025-02-28T21:10:28.147234Z"}},"outputs":[{"name":"stdout","text":"You are eligible for Behavioral and Social Sciences Grant. You are eligible because you have a GPA of 3.01 in Anthropology and an income of $464150. Your field of study is in Behavioral and Social Sciences. You meet the requirements for Behavioral and Social Sciences Grant.\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Run another prompt\nresponse = generate_text(random.choice(student_prompts))\nprint(extract_answer(response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:10:44.021475Z","iopub.execute_input":"2025-02-28T21:10:44.021805Z","iopub.status.idle":"2025-02-28T21:10:53.681879Z","shell.execute_reply.started":"2025-02-28T21:10:44.021777Z","shell.execute_reply":"2025-02-28T21:10:53.680692Z"}},"outputs":[{"name":"stdout","text":"Yes, you are eligible for the STEM Excellence Award, which provides $10,000 for each year of study. This award is only available to students who are pursuing a degree in a STEM field and have a GPA of 3.0 or higher. You are also eligible for other scholarships and grants based on your income and financial need. Please contact the Financial Aid Office for more information on available aid options.\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## Step 8: Merge and Save the Model","metadata":{}},{"cell_type":"code","source":"# Merge LoRA weights with the base model and save the final merged version\nfine_tuned_model_storage_path = \"./llama3-student-aid-finetuned\"\nmodel = model.merge_and_unload()\nmodel.save_pretrained(fine_tuned_model_storage_path)\ntokenizer.save_pretrained(fine_tuned_model_storage_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T21:28:08.907619Z","iopub.execute_input":"2025-02-28T21:28:08.908017Z","iopub.status.idle":"2025-02-28T21:28:17.675497Z","shell.execute_reply.started":"2025-02-28T21:28:08.907990Z","shell.execute_reply":"2025-02-28T21:28:17.674285Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:85: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"('./llama3-student-aid-finetuned/tokenizer_config.json',\n './llama3-student-aid-finetuned/special_tokens_map.json',\n './llama3-student-aid-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":66},{"cell_type":"markdown","source":"## Step 8: Evaluate Model\n\n### TBD - requires uploading separate dataset","metadata":{}}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned PHI 4 Mini - Financial Aid Chat Usecase \n",
    "\n",
    "This notebook pulls down the Microsoft PHI 4 Mini model from HuggingFace and leverages a pre-made synthetic dataset comprising of fictitious student records. LoRA adapters were used to fine-tune the model efficiently, reducing memory usage by keeping the base model frozen and training only additional adapter layers. No real data is used in this dataset and the code is free from any conflict of interest to the best of the author's knowledge.\n",
    "\n",
    "View on Kaggle: [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/code/edyvision/fine-tuned-phi-4-mini-financial-aid-chat-usecase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U torch transformers psutil GPUtil seaborn matplotlib fvcore evaluate  prettytable peft accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import dependencies and deal with memory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-02T03:22:53.760672Z",
     "iopub.status.busy": "2025-04-02T03:22:53.760350Z",
     "iopub.status.idle": "2025-04-02T03:23:16.756493Z",
     "shell.execute_reply": "2025-04-02T03:23:16.755600Z",
     "shell.execute_reply.started": "2025-04-02T03:22:53.760631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "# For LLM\n",
    "from peft import get_peft_model, LoraConfig, PeftModel, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:24:26.306660Z",
     "iopub.status.busy": "2025-04-02T03:24:26.306366Z",
     "iopub.status.idle": "2025-04-02T03:24:26.394475Z",
     "shell.execute_reply": "2025-04-02T03:24:26.393414Z",
     "shell.execute_reply.started": "2025-04-02T03:24:26.306639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device (local is MPS, deployed will almost always be CUDA)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:34.409137Z",
     "iopub.status.busy": "2025-04-02T03:23:34.408810Z",
     "iopub.status.idle": "2025-04-02T03:23:34.514654Z",
     "shell.execute_reply": "2025-04-02T03:23:34.513984Z",
     "shell.execute_reply.started": "2025-04-02T03:23:34.409104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import HuggingFace Secret\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secret_label = \"HUGGINGFACE_TOKEN\"\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:36.027269Z",
     "iopub.status.busy": "2025-04-02T03:23:36.026972Z",
     "iopub.status.idle": "2025-04-02T03:23:36.030949Z",
     "shell.execute_reply": "2025-04-02T03:23:36.030024Z",
     "shell.execute_reply.started": "2025-04-02T03:23:36.027246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "local_storage_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Datasets and Synthetic Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:38.319891Z",
     "iopub.status.busy": "2025-04-02T03:23:38.319571Z",
     "iopub.status.idle": "2025-04-02T03:23:38.355485Z",
     "shell.execute_reply": "2025-04-02T03:23:38.354779Z",
     "shell.execute_reply.started": "2025-04-02T03:23:38.319866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Data to Fine Tune PHI-4\n",
    "root_path = '/kaggle/input/student-chat-data/student_chat_ai_data'\n",
    "train_df = pd.read_csv(f\"{root_path}/train_data.csv\", index_col=0)\n",
    "eval_df = pd.read_csv(f\"{root_path}/eval_data.csv\", index_col=0)\n",
    "synthetic_student_population_df = pd.read_csv(f\"{root_path}/synthetic_population_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:40.747292Z",
     "iopub.status.busy": "2025-04-02T03:23:40.746996Z",
     "iopub.status.idle": "2025-04-02T03:23:40.758848Z",
     "shell.execute_reply": "2025-04-02T03:23:40.758163Z",
     "shell.execute_reply.started": "2025-04-02T03:23:40.747269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>user_input</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student Linda Bailey (ID: S00001) has a GPA of...</td>\n",
       "      <td>What financial aid do I qualify for?</td>\n",
       "      <td>You qualify for the Need-Based Grant because y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student John Doe (ID: S00002) has a GPA of 3.8...</td>\n",
       "      <td>What financial aid am I eligible for?</td>\n",
       "      <td>John Doe qualifies for the Merit-Based Scholar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student Sarah Carter (ID: S00003) has a GPA of...</td>\n",
       "      <td>Can I apply for any financial aid?</td>\n",
       "      <td>Yes! You qualify for the STEM Excellence Award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student John Doe (ID: S10123) has a GPA of 2.3...</td>\n",
       "      <td>What am I to be eligible for financial aid?</td>\n",
       "      <td>Due to your income, you do not qualify for a n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Student Linda Bailey (ID: S00001) has a GPA of...   \n",
       "1  Student John Doe (ID: S00002) has a GPA of 3.8...   \n",
       "2  Student Sarah Carter (ID: S00003) has a GPA of...   \n",
       "3  Student John Doe (ID: S10123) has a GPA of 2.3...   \n",
       "\n",
       "                                    user_input  \\\n",
       "0         What financial aid do I qualify for?   \n",
       "1        What financial aid am I eligible for?   \n",
       "2           Can I apply for any financial aid?   \n",
       "3  What am I to be eligible for financial aid?   \n",
       "\n",
       "                                              answer  \n",
       "0  You qualify for the Need-Based Grant because y...  \n",
       "1  John Doe qualifies for the Merit-Based Scholar...  \n",
       "2  Yes! You qualify for the STEM Excellence Award...  \n",
       "3  Due to your income, you do not qualify for a n...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:43.430654Z",
     "iopub.status.busy": "2025-04-02T03:23:43.430347Z",
     "iopub.status.idle": "2025-04-02T03:23:43.438702Z",
     "shell.execute_reply": "2025-04-02T03:23:43.437854Z",
     "shell.execute_reply.started": "2025-04-02T03:23:43.430632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>user_input</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student Emily Carter (ID: S00023) has a GPA of...</td>\n",
       "      <td>Am I eligible for any financial aid?</td>\n",
       "      <td>Yes, you qualify for the STEM Excellence Award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student Mark Smith (ID: S00045) has a GPA of 2...</td>\n",
       "      <td>Can I get coverage with my education program?</td>\n",
       "      <td>Unfortunately, you do not qualify for any fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student Olivia Jones (ID: S00030) has a GPA of...</td>\n",
       "      <td>Which financial aid can do I qualify for?</td>\n",
       "      <td>You qualify for the STEM Excellence Award sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student Daniel Thompson (ID: S00051) has a GPA...</td>\n",
       "      <td>What financial aid options do I have?</td>\n",
       "      <td>You are eligible for the Need-Based Grant sinc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Student Emily Carter (ID: S00023) has a GPA of...   \n",
       "1  Student Mark Smith (ID: S00045) has a GPA of 2...   \n",
       "2  Student Olivia Jones (ID: S00030) has a GPA of...   \n",
       "3  Student Daniel Thompson (ID: S00051) has a GPA...   \n",
       "\n",
       "                                      user_input  \\\n",
       "0           Am I eligible for any financial aid?   \n",
       "1  Can I get coverage with my education program?   \n",
       "2      Which financial aid can do I qualify for?   \n",
       "3          What financial aid options do I have?   \n",
       "\n",
       "                                              answer  \n",
       "0  Yes, you qualify for the STEM Excellence Award...  \n",
       "1  Unfortunately, you do not qualify for any fina...  \n",
       "2  You qualify for the STEM Excellence Award sinc...  \n",
       "3  You are eligible for the Need-Based Grant sinc...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:45.568958Z",
     "iopub.status.busy": "2025-04-02T03:23:45.568647Z",
     "iopub.status.idle": "2025-04-02T03:23:45.580752Z",
     "shell.execute_reply": "2025-04-02T03:23:45.579740Z",
     "shell.execute_reply.started": "2025-04-02T03:23:45.568932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>gpa</th>\n",
       "      <th>field_of_study</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S00001</td>\n",
       "      <td>Marc Solomon</td>\n",
       "      <td>2.53</td>\n",
       "      <td>Anthropology</td>\n",
       "      <td>73706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S00002</td>\n",
       "      <td>Suzanne Kidd</td>\n",
       "      <td>2.50</td>\n",
       "      <td>Business Administration</td>\n",
       "      <td>22259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S00003</td>\n",
       "      <td>Michelle Griffin</td>\n",
       "      <td>2.42</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>31192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S00004</td>\n",
       "      <td>Hannah Dixon</td>\n",
       "      <td>2.05</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>530431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S00005</td>\n",
       "      <td>Mary Harris</td>\n",
       "      <td>3.13</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>370142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>S00996</td>\n",
       "      <td>Nicole Clay</td>\n",
       "      <td>3.97</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>353792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>S00997</td>\n",
       "      <td>Tammy Rose</td>\n",
       "      <td>3.34</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>414283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>S00998</td>\n",
       "      <td>Jeff Alvarez</td>\n",
       "      <td>3.96</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>450287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>S00999</td>\n",
       "      <td>Thomas Lowe</td>\n",
       "      <td>3.02</td>\n",
       "      <td>Business Administration</td>\n",
       "      <td>54775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>S01000</td>\n",
       "      <td>Veronica Barker</td>\n",
       "      <td>3.29</td>\n",
       "      <td>Anthropology</td>\n",
       "      <td>270061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              name   gpa           field_of_study  income\n",
       "0    S00001      Marc Solomon  2.53             Anthropology   73706\n",
       "1    S00002      Suzanne Kidd  2.50  Business Administration   22259\n",
       "2    S00003  Michelle Griffin  2.42        Political Science   31192\n",
       "3    S00004      Hannah Dixon  2.05              Engineering  530431\n",
       "4    S00005       Mary Harris  3.13                Chemistry  370142\n",
       "..      ...               ...   ...                      ...     ...\n",
       "995  S00996       Nicole Clay  3.97                Chemistry  353792\n",
       "996  S00997        Tammy Rose  3.34         Computer Science  414283\n",
       "997  S00998      Jeff Alvarez  3.96               Psychology  450287\n",
       "998  S00999       Thomas Lowe  3.02  Business Administration   54775\n",
       "999  S01000   Veronica Barker  3.29             Anthropology  270061\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_student_population_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load PHI 4 Mini Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:52.221277Z",
     "iopub.status.busy": "2025-04-02T03:23:52.220980Z",
     "iopub.status.idle": "2025-04-02T03:23:52.225094Z",
     "shell.execute_reply": "2025-04-02T03:23:52.224351Z",
     "shell.execute_reply.started": "2025-04-02T03:23:52.221253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:48.774259Z",
     "iopub.status.busy": "2025-04-02T03:23:48.773948Z",
     "iopub.status.idle": "2025-04-02T03:23:48.781403Z",
     "shell.execute_reply": "2025-04-02T03:23:48.780488Z",
     "shell.execute_reply.started": "2025-04-02T03:23:48.774237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StudentDialogueDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, device, model_type, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "\n",
    "    def format_instruct(self, example):\n",
    "        \"\"\"Format dataset for instruction tuning using system prompt, context, user input, and answer.\"\"\"\n",
    "        system_prompt = \"### System:\\nYou are an AI financial aid evaluator. Answer the user's question based on the provided student's profile.\\n\"\n",
    "\n",
    "        context = f\"### Context:\\n{example['context']}\\n\" if example[\"context\"] else \"\"\n",
    "        instruction = f\"### User Input:\\n{example['user_input']}\\n\"\n",
    "        response = f\"### Response:\\n{example['answer']}\\n\"\n",
    "\n",
    "        return system_prompt + context + instruction + response\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Choose formatting based on `model_type`\n",
    "        if self.model_type == \"instruct\":\n",
    "            formatted_text = self.format_instruct(item)\n",
    "        else:  # \"causal\"\n",
    "            formatted_text = (\n",
    "                item[\"context\"] + \" \" + item[\"user_input\"] + \" \" + item[\"answer\"]\n",
    "            )\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            formatted_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0).to(self.device)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0).to(self.device)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone(),  # Causal LM: Input = Label (next-token prediction)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:23:54.082054Z",
     "iopub.status.busy": "2025-04-02T03:23:54.081725Z",
     "iopub.status.idle": "2025-04-02T03:23:54.093638Z",
     "shell.execute_reply": "2025-04-02T03:23:54.092641Z",
     "shell.execute_reply.started": "2025-04-02T03:23:54.082031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CausalModelTrainer:\n",
    "    \"\"\"Causal Model Trainer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        dataset,\n",
    "        logger,\n",
    "        device=\"mps\",\n",
    "        huggingface_token=None,\n",
    "    ):\n",
    "        \"\"\"init\"\"\"\n",
    "\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.logger = logger\n",
    "        self.huggingface_token = huggingface_token\n",
    "        self.device = device\n",
    "\n",
    "    def load_model(\n",
    "        self,\n",
    "        lora_target_modules=None,\n",
    "        compute_dtype=\"float16\",\n",
    "        use_cache=False,\n",
    "        device_map=\"auto\",\n",
    "        apply_lora=True,\n",
    "    ):\n",
    "        \"\"\"Loads the base model and applies LoRA.\"\"\"\n",
    "\n",
    "        if not lora_target_modules and apply_lora:\n",
    "            raise ValueError(\"Please specify target modules for LoRA.\")\n",
    "\n",
    "        self.logger.info(f\"Model will be moved to {self.device}\")\n",
    "\n",
    "        # Record the start time to measure the loading duration\n",
    "        time_start = time()\n",
    "\n",
    "        # Load the pre-trained base model with specified configurations\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                f\"{model_path}\",\n",
    "                torch_dtype=compute_dtype,\n",
    "                use_cache=use_cache,\n",
    "                device_map=self.device,\n",
    "                token=self.huggingface_token,\n",
    "            )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"{model_path}\",\n",
    "                token=self.huggingface_token,\n",
    "            )\n",
    "\n",
    "        self.logger.info(f\"Model loaded in {time() - time_start:.2f} seconds.\")\n",
    "\n",
    "        if apply_lora:\n",
    "            # Apply LoRA to the model\n",
    "            self.model = self.apply_lora(\n",
    "                model=self.model, target_modules=lora_target_modules\n",
    "            )\n",
    "\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def apply_lora(self, model, target_modules, print_trainable_params=True):\n",
    "        \"\"\"Applies LoRA.\"\"\"\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # Rank (smaller = more efficient)\n",
    "            lora_alpha=64,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=target_modules,  # Specify the correct layers\n",
    "            task_type=TaskType.CAUSAL_LM,  # For language modeling\n",
    "        )\n",
    "\n",
    "        # Apply LoRA to model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.to(self.device)\n",
    "\n",
    "        if print_trainable_params:\n",
    "            # After applying LoRA, confirm which layers are trainable\n",
    "            model.print_trainable_parameters()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_dataset(self, dataset_class, test_train_split=0.2):\n",
    "        \"\"\"Loads dataset and applies text preprocessing.\"\"\"\n",
    "        train_data, test_data = train_test_split(\n",
    "            self.dataset,\n",
    "            test_size=1 - test_train_split,\n",
    "            stratify=self.dataset[\"output\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_data = train_data.reset_index(drop=True)\n",
    "        test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "        self.train_dataset = dataset_class(\n",
    "            train_data.to_dict(orient=\"records\"),\n",
    "            self.tokenizer,\n",
    "            labels=self.labels,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        self.test_dataset = dataset_class(\n",
    "            test_data.to_dict(orient=\"records\"),\n",
    "            self.tokenizer,\n",
    "            labels=self.labels,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        return self.train_dataset, self.test_dataset\n",
    "\n",
    "    def load_lora_adapters(self, base_model, adapter_path, merge_adapters=True):\n",
    "        \"\"\"Loads the base model and LoRA adapters\"\"\"\n",
    "        # Load the base model\n",
    "        self.model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "        if merge_adapters:\n",
    "            # Merge the LoRA adapters into the base model\n",
    "            self.model = self.model.merge_and_unload()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_arguments,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        output_dir=\"./dist/instruct_lora/checkpoints\",\n",
    "        adapter_path=\"./dist/instruct_lora\",\n",
    "        return_tensors=\"pt\",\n",
    "        pad_to_multiple_of=8,\n",
    "    ):\n",
    "        \"\"\"Runs the training process using Hugging Face Trainer.\"\"\"\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_arguments,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save LoRA adapter files\n",
    "        trainer.save_model(adapter_path)\n",
    "        self.logger.info(f\"Model LoRA adapter files saved at {adapter_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:24:34.494239Z",
     "iopub.status.busy": "2025-04-02T03:24:34.493940Z",
     "iopub.status.idle": "2025-04-02T03:25:10.261794Z",
     "shell.execute_reply": "2025-04-02T03:25:10.260944Z",
     "shell.execute_reply.started": "2025-04-02T03:24:34.494216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9cde1205cd47f3a60759cd831f9046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e779bce421416fa04d0d9524aa6ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89380d2966c243969dec8d53ea2053af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaeb1c69b934f4197fb2b44f2dad728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337cacbf5c4b43b992fb139572e71559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab4023eb6c4459ba1ccd361c5377c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c44de6cc07b4fbfa0fba5cd04672b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9097848ae7b43158ed9bfe91a0a803b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4487bc6947ab4f45b7fb86ad28f292d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0675cf0bf5784ea888f668777bd1c0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3358d21a38844dba516ed1089261115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec7d592e94e4f74978e754eb23855aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,145,728 || all params: 3,839,167,488 || trainable%: 0.0819\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Import Base Model (used for all use cases)\n",
    "model_name = \"Phi-4-mini-instruct\"\n",
    "model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model_trainer = CausalModelTrainer(\n",
    "    model_name=model_path,\n",
    "    dataset=train_df,\n",
    "    logger=logger,\n",
    "    device=device,\n",
    "    huggingface_token=secret_value\n",
    ")\n",
    "\n",
    "model, tokenizer = model_trainer.load_model(\n",
    "    apply_lora=True,\n",
    "    lora_target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:26:16.800608Z",
     "iopub.status.busy": "2025-04-02T03:26:16.800228Z",
     "iopub.status.idle": "2025-04-02T03:26:16.806146Z",
     "shell.execute_reply": "2025-04-02T03:26:16.805252Z",
     "shell.execute_reply.started": "2025-04-02T03:26:16.800580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = StudentDialogueDataset(train_df.to_dict(orient='records'), tokenizer, model_type=\"instruct\", device=device)\n",
    "eval_dataset = StudentDialogueDataset(eval_df.to_dict(orient='records'), tokenizer, model_type=\"instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:26:19.007781Z",
     "iopub.status.busy": "2025-04-02T03:26:19.007482Z",
     "iopub.status.idle": "2025-04-02T03:26:19.011470Z",
     "shell.execute_reply": "2025-04-02T03:26:19.010489Z",
     "shell.execute_reply.started": "2025-04-02T03:26:19.007759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoints_dir = f\"{local_storage_dir}/models/phi4_chat_lora/checkpoints\"\n",
    "lora_adapter_path = f\"{local_storage_dir}/models/phi4_chat_lora/lora_adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:26:21.551674Z",
     "iopub.status.busy": "2025-04-02T03:26:21.551291Z",
     "iopub.status.idle": "2025-04-02T03:26:21.892970Z",
     "shell.execute_reply": "2025-04-02T03:26:21.891972Z",
     "shell.execute_reply.started": "2025-04-02T03:26:21.551644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# The time has come, empty cache\n",
    "import gc\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "clear_cuda() if torch.cuda.is_available() else torch.mps.empty_cache() if torch.backends.mps.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:26:24.023224Z",
     "iopub.status.busy": "2025-04-02T03:26:24.022933Z",
     "iopub.status.idle": "2025-04-02T03:26:42.436483Z",
     "shell.execute_reply": "2025-04-02T03:26:42.435473Z",
     "shell.execute_reply.started": "2025-04-02T03:26:24.023202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.738935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.745264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.736712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.726398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.353900</td>\n",
       "      <td>7.690777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoints_dir,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size for MPS stability\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,  # Use `bf16` if on MPS, it supports it\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",\n",
    "    warmup_steps=100,  # Add a warmup phase\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True, # if you want the models at the end, set to 'True'\n",
    ")\n",
    "\n",
    "model_trainer.train(\n",
    "    training_arguments=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    output_dir=checkpoints_dir,\n",
    "    adapter_path=lora_adapter_path,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:26:46.943671Z",
     "iopub.status.busy": "2025-04-02T03:26:46.943285Z",
     "iopub.status.idle": "2025-04-02T03:26:47.416432Z",
     "shell.execute_reply": "2025-04-02T03:26:47.415415Z",
     "shell.execute_reply.started": "2025-04-02T03:26:46.943640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Empty cache ahead of merge and unload\n",
    "clear_cuda() if torch.cuda.is_available() else torch.mps.empty_cache() if torch.backends.mps.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:27:01.401447Z",
     "iopub.status.busy": "2025-04-02T03:27:01.401015Z",
     "iopub.status.idle": "2025-04-02T03:27:34.929518Z",
     "shell.execute_reply": "2025-04-02T03:27:34.928380Z",
     "shell.execute_reply.started": "2025-04-02T03:27:01.401396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39759cc0e78d43be9b1f64489db73fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged Phi-4-mini-instruct model saved (base model + LoRA adapters)!\n",
      "Model saved to /kaggle/working/models/phi4_mini/chat/v1.0\n"
     ]
    }
   ],
   "source": [
    "# Load Base Model\n",
    "new_model_trainer = CausalModelTrainer(\n",
    "    model_name=model_path,\n",
    "    dataset=train_df,\n",
    "    logger=logger,\n",
    "    device=device,\n",
    "    huggingface_token=secret_value\n",
    ")\n",
    "base_model, tokenizer = new_model_trainer.load_model(apply_lora=False)\n",
    "\n",
    "# Load LoRA Adapters\n",
    "model = new_model_trainer.load_lora_adapters(\n",
    "    base_model,\n",
    "    lora_adapter_path,\n",
    "    merge_adapters=True  # Merge adapters into base model\n",
    "    )\n",
    "\n",
    "# Save the Fully Fine-Tuned Model\n",
    "tuned_version = 1\n",
    "tuned_model_name = \"chat\"\n",
    "tuned_model_storage_dir = f\"{local_storage_dir}/models/phi4_mini/{tuned_model_name}\"\n",
    "tuned_model_storage_path = f\"{tuned_model_storage_dir}/v{tuned_version}.0\"\n",
    "s3_model_storage_dir = f\"phi4_mini/{tuned_model_name}/v{tuned_version}.0\"\n",
    "\n",
    "model.save_pretrained(tuned_model_storage_path)\n",
    "tokenizer.save_pretrained(tuned_model_storage_path)\n",
    "\n",
    "print(f\"\\nMerged {model_name} model saved (base model + LoRA adapters)!\")\n",
    "print(f\"Model saved to {tuned_model_storage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, that is all that is required for full download, fine-tune, and saving of fine-tuned model. The remainder of this notebook is on using the model for the student chat use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess Synthetic Student Data\n",
    "Using the synthetic student dataset, construct prompts to test out the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:00.680704Z",
     "iopub.status.busy": "2025-04-02T03:28:00.680377Z",
     "iopub.status.idle": "2025-04-02T03:28:00.691687Z",
     "shell.execute_reply": "2025-04-02T03:28:00.690804Z",
     "shell.execute_reply.started": "2025-04-02T03:28:00.680678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 student records available\n",
      "\n",
      "Data record shape:\n",
      "\n",
      "{'id': 'S00001', 'name': 'Marc Solomon', 'gpa': 2.53, 'field_of_study': 'Anthropology', 'income': 73706}\n"
     ]
    }
   ],
   "source": [
    "fake_students = synthetic_student_population_df.to_dict(orient=\"records\")\n",
    "\n",
    "print(f\"{len(fake_students)} student records available\")\n",
    "print(\"\\nData record shape:\\n\")\n",
    "print(fake_students[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:02.804108Z",
     "iopub.status.busy": "2025-04-02T03:28:02.803805Z",
     "iopub.status.idle": "2025-04-02T03:28:02.811914Z",
     "shell.execute_reply": "2025-04-02T03:28:02.810989Z",
     "shell.execute_reply.started": "2025-04-02T03:28:02.804084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "@staticmethod\n",
    "def determine_financial_aid_eligibility(gpa, field_of_study, income):\n",
    "    \"\"\"\n",
    "    Uses a student record to determine coverage against some fictitious\n",
    "    parameters\n",
    "    \"\"\"\n",
    "    # Dummy logic for financial aid eligibility\n",
    "    financial_aid = []\n",
    "    requirements = []\n",
    "\n",
    "    if gpa >= 3.6:\n",
    "        requirements.append(\"GPA â‰¥ 3.6\")\n",
    "    if income <= 35000:\n",
    "        requirements.append(\"Income â‰¤ $35,000\")\n",
    "    if field_of_study in [\n",
    "        \"Computer Science\",\n",
    "        \"Engineering\",\n",
    "        \"Mathematics\",\n",
    "        \"IT\",\n",
    "        \"Statistics\",\n",
    "    ]:\n",
    "        financial_aid.append(\"STEM Excellence Award\")\n",
    "        requirements.append(\"Field of Study in STEM\")\n",
    "    if field_of_study in [\"Psychology\", \"Sociology\", \"Social Work\", \"Anthropology\"]:\n",
    "        financial_aid.append(\"Behavioral and Social Sciences Grant\")\n",
    "        requirements.append(\"Field of Study in Behavioral and Social Sciences\")\n",
    "    if field_of_study in [\"Paralegal\", \"Law\"]:\n",
    "        financial_aid.append(\"Legal Studies Full Ride\")\n",
    "        requirements.append(\"Field of Study in Legal Studies\")\n",
    "\n",
    "    return requirements, financial_aid\n",
    "\n",
    "def retrieve_financial_aid_knowledge(student_id: Optional[str] = None, student_name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Retrieves financial aid eligibility knowledge for a student.\n",
    "    \"\"\"\n",
    "    if student_id:\n",
    "        student = next((item for item in fake_students if item[\"id\"] == student_id), None)\n",
    "    if student_name:\n",
    "        student = next((item for item in fake_students if item[\"name\"] == student_name), None)\n",
    "\n",
    "    if not student:\n",
    "        return None\n",
    "\n",
    "    requirements, financial_aid = determine_financial_aid_eligibility(\n",
    "        student[\"gpa\"], student[\"field_of_study\"], student[\"income\"]\n",
    "        )\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"id\": student[\"id\"],\n",
    "        \"name\": student[\"name\"],\n",
    "        \"gpa\": student[\"gpa\"],\n",
    "        \"field_of_study\": student[\"field_of_study\"],\n",
    "        \"income\": student[\"income\"],\n",
    "        \"financial_aid\": financial_aid,\n",
    "        \"requirements\": requirements,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:06.189096Z",
     "iopub.status.busy": "2025-04-02T03:28:06.188782Z",
     "iopub.status.idle": "2025-04-02T03:28:06.194310Z",
     "shell.execute_reply": "2025-04-02T03:28:06.193385Z",
     "shell.execute_reply.started": "2025-04-02T03:28:06.189071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'S00001', 'name': 'Marc Solomon', 'gpa': 2.53, 'field_of_study': 'Anthropology', 'income': 73706, 'financial_aid': ['Behavioral and Social Sciences Grant'], 'requirements': ['Field of Study in Behavioral and Social Sciences']}\n",
      "Student Marc Solomon (ID: S00001) has a GPA of 2.53 in Anthropology and an income of $73706. Eligible financial aid: Behavioral and Social Sciences Grant. Requirements met: Field of Study in Behavioral and Social Sciences.\n"
     ]
    }
   ],
   "source": [
    "def format(student: dict) -> str:\n",
    "    \"\"\"\n",
    "    Converts the retrieved student and financial aid knowledge into an LLM-readable prompt.\n",
    "    \"\"\"\n",
    "    return f\"Student {student['name']} (ID: {student['id']}) has a GPA of {student['gpa']} in {student['field_of_study']} \" + \\\n",
    "        f\"and an income of ${student['income']}. \" + \\\n",
    "        f\"Eligible financial aid: {', '.join(student['financial_aid']) if student.get('financial_aid') else 'None'}. \" + \\\n",
    "        f\"Requirements met: {', '.join(student['requirements']) if student.get('requirements') else 'None'}.\"\n",
    "\n",
    "\n",
    "retrieved_knowledge = retrieve_financial_aid_knowledge(\"S00001\")\n",
    "print(retrieved_knowledge)  # Data as-is\n",
    "print(format(retrieved_knowledge)) # Data formatted for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Pre-build all prompts using synthetic population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:08.809551Z",
     "iopub.status.busy": "2025-04-02T03:28:08.809179Z",
     "iopub.status.idle": "2025-04-02T03:28:08.814291Z",
     "shell.execute_reply": "2025-04-02T03:28:08.813282Z",
     "shell.execute_reply.started": "2025-04-02T03:28:08.809523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "def build_student_prompt(user_input: str, student_id: Optional[str] = None) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Builds student chat prompt for LLM and uses RAG to keep responses properly informed for\n",
    "    the given student with student_id.\n",
    "    \"\"\"\n",
    "\n",
    "    # student id available and valid, proceed with lookup\n",
    "    knowledge = format(retrieve_financial_aid_knowledge(student_id))\n",
    "\n",
    "    # Retrun both the knowledge rertieved and the prompt\n",
    "    return (\n",
    "        knowledge,\n",
    "        f\"\"\"\n",
    "        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\n{knowledge}\\n\\n### User's Question:\\n{user_input}\\n\\n###Answer: \n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:14.530699Z",
     "iopub.status.busy": "2025-04-02T03:28:14.530373Z",
     "iopub.status.idle": "2025-04-02T03:28:14.571704Z",
     "shell.execute_reply": "2025-04-02T03:28:14.570861Z",
     "shell.execute_reply.started": "2025-04-02T03:28:14.530673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Marc Solomon (ID: S00001) has a GPA of 2.53 in Anthropology and an income of $73706. Eligible financial aid: Behavioral and Social Sciences Grant. Requirements met: Field of Study in Behavioral and Social Sciences.\\n\\n### User's Question:\\nWhat grants can I apply for?\\n\\n###Answer: \\n        \",\n",
       " \"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Suzanne Kidd (ID: S00002) has a GPA of 2.5 in Business Administration and an income of $22259. Eligible financial aid: None. Requirements met: Income â‰¤ $35,000.\\n\\n### User's Question:\\nWhat GPA do I need need to qualify for financial aid?\\n\\n###Answer: \\n        \",\n",
       " \"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Michelle Griffin (ID: S00003) has a GPA of 2.42 in Political Science and an income of $31192. Eligible financial aid: None. Requirements met: Income â‰¤ $35,000.\\n\\n### User's Question:\\nWhat GPA do I need need to qualify for financial aid?\\n\\n###Answer: \\n        \",\n",
       " \"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Hannah Dixon (ID: S00004) has a GPA of 2.05 in Engineering and an income of $530431. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\\n\\n### User's Question:\\nWhat grants can I apply for?\\n\\n###Answer: \\n        \",\n",
       " \"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Mary Harris (ID: S00005) has a GPA of 3.13 in Chemistry and an income of $370142. Eligible financial aid: None. Requirements met: None.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define test dataset (Example questions)\n",
    "def build_test_sentences():\n",
    "    test_sentences = [\n",
    "        f\"What GPA do I need need to qualify for financial aid?\",\n",
    "        f\"Am I eligible for scholarships?\",\n",
    "        f\"What grants can I apply for?\",\n",
    "    ]\n",
    "\n",
    "    return test_sentences\n",
    "\n",
    "student_prompts = []\n",
    "\n",
    "for student in fake_students:\n",
    "    test_sentence = random.choice(build_test_sentences())\n",
    "    knowledge_retrieved, prompt = build_student_prompt(user_input=test_sentence, student_id=student[\"id\"])\n",
    "    student_prompts.append(prompt)\n",
    "\n",
    "student_prompts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set to start testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run custom student inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:19.052697Z",
     "iopub.status.busy": "2025-04-02T03:28:19.052388Z",
     "iopub.status.idle": "2025-04-02T03:28:20.621577Z",
     "shell.execute_reply": "2025-04-02T03:28:20.620756Z",
     "shell.execute_reply.started": "2025-04-02T03:28:19.052674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:20.623234Z",
     "iopub.status.busy": "2025-04-02T03:28:20.622913Z",
     "iopub.status.idle": "2025-04-02T03:28:20.628876Z",
     "shell.execute_reply": "2025-04-02T03:28:20.628090Z",
     "shell.execute_reply.started": "2025-04-02T03:28:20.623207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        ### Role: You are a financial aid evaluator. Answer the user's question based on the provided student's profile. Do not repeat the question or context, do not add any data not available regarding funding amount or renewals. Make sure to use the eligible financial aid and requirements met to inform their eligibility. \\n\\n### Context:\\nStudent Steven Kaiser (ID: S00572) has a GPA of 2.97 in Mathematics and an income of $136494. Eligible financial aid: STEM Excellence Award. Requirements met: Field of Study in STEM.\\n\\n### User's Question:\\nAm I eligible for scholarships?\\n\\n###Answer: \\n        \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select first prompt\n",
    "first_prompt = random.choice(student_prompts)\n",
    "first_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:23.121532Z",
     "iopub.status.busy": "2025-04-02T03:28:23.121035Z",
     "iopub.status.idle": "2025-04-02T03:28:23.128960Z",
     "shell.execute_reply": "2025-04-02T03:28:23.127711Z",
     "shell.execute_reply.started": "2025-04-02T03:28:23.121488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_answer(response_text):\n",
    "    \"\"\"\n",
    "    Extracts the generated answer from the LLM response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match the last \"Answer:\" followed by actual content\n",
    "    match = re.search(\n",
    "        r\"(?i)(?:Your response:|Answer:)\\s*\\n*(.*?)(?=\\n\\n|\\Z)\",\n",
    "        response_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "        extracted_answer = match.group(1).strip()\n",
    "        return extracted_answer\n",
    "    else:\n",
    "        return \"No answer found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:34.355844Z",
     "iopub.status.busy": "2025-04-02T03:28:34.355521Z",
     "iopub.status.idle": "2025-04-02T03:28:35.131160Z",
     "shell.execute_reply": "2025-04-02T03:28:35.130271Z",
     "shell.execute_reply.started": "2025-04-02T03:28:34.355814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Steven Kaiser is eligible for the STEM Excellence Award.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Run first prompt\n",
    "response = generate_text(first_prompt)\n",
    "print(extract_answer(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:28:37.512699Z",
     "iopub.status.busy": "2025-04-02T03:28:37.512359Z",
     "iopub.status.idle": "2025-04-02T03:28:39.370543Z",
     "shell.execute_reply": "2025-04-02T03:28:39.369803Z",
     "shell.execute_reply.started": "2025-04-02T03:28:37.512672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Smith is not eligible for any grants as there are no eligible financial aids listed for his profile. His GPA meets the requirement, but the absence of eligible financial aids means no grants can be applied for.\n"
     ]
    }
   ],
   "source": [
    "# Run another prompt\n",
    "response = generate_text(random.choice(student_prompts))\n",
    "print(extract_answer(response))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6758176,
     "sourceId": 10876989,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
